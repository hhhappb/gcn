# æ–‡ä»¶å: model/sgt_net8.10.py(8.10)
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.nn.init as init
import math
import numpy as np
from einops import rearrange
from torch.autograd import Variable
from .tem_mixf import Temporal_MixFormer
from timm.layers.drop import DropPath
from timm.layers.weight_init import trunc_normal_
from .precomputed_hop_matrices import get_precomputed_hop_matrix

def import_class(name):
    components = name.split('.')
    mod = __import__(components[0])
    for comp in components[1:]:
        mod = getattr(mod, comp)
    return mod

def conv_init(conv):
    if conv.weight is not None:
        nn.init.kaiming_normal_(conv.weight, mode='fan_out')
    if conv.bias is not None:
        nn.init.constant_(conv.bias, 0)

def bn_init(bn, scale):
    nn.init.constant_(bn.weight, scale)
    nn.init.constant_(bn.bias, 0)

def weights_init(m):
    classname = m.__class__.__name__
    if classname.find('Conv') != -1:
        if hasattr(m, 'weight'):
            nn.init.kaiming_normal_(m.weight, mode='fan_out')
        if hasattr(m, 'bias') and m.bias is not None and isinstance(m.bias, torch.Tensor):
            nn.init.constant_(m.bias, 0)
    elif classname.find('BatchNorm') != -1:
        if hasattr(m, 'weight') and m.weight is not None:
            m.weight.data.normal_(1.0, 0.02)
        if hasattr(m, 'bias') and m.bias is not None:
            m.bias.data.fill_(0)

class AttentionLayer(nn.Module):
    def __init__(self, d_model, n_heads, dropout_rate=0.0, hop_matrix=None):
        super().__init__()
        assert d_model % n_heads == 0, "d_model must be divisible by n_heads"
        
        self.n_heads = n_heads
        self.head_dim = d_model // n_heads
        self.scale = self.head_dim ** -0.5
        
        # èåˆQKVæŠ•å½±ä¼˜åŒ–
        self.qkv_proj = nn.Linear(d_model, d_model * 3, bias=False)
        self.out_proj = nn.Linear(d_model, d_model)
        self.dropout_p = dropout_rate
        self.attn_drop = nn.Dropout(dropout_rate)

        # æ¯å¤´ç‹¬ç«‹çš„RPE bias
        if hop_matrix is not None:
            self.register_buffer('hop_matrix', hop_matrix)
            max_hop = hop_matrix.max().item()
            # æ¯ä¸ªå¤´å¯¹æ¯ç§è·ç¦»éƒ½æœ‰ç‹¬ç«‹çš„æ ‡é‡åç½®
            self.rpe_bias = nn.Parameter(torch.zeros(self.n_heads, max_hop + 1))
        else:
            self.rpe_bias = None

    def forward(self, x):
        B, V, D = x.shape

        # 1.èåˆçš„QKVæŠ•å½±
        qkv = self.qkv_proj(x)  # [B, V, 3*D]
        q, k, v = qkv.chunk(3, dim=-1)  # æ¯ä¸ªéƒ½æ˜¯ [B, V, D]

        # 2. è½¬æ¢ä¸ºå¤šå¤´æ ¼å¼
        q = q.view(B, V, self.n_heads, self.head_dim).transpose(1, 2)  # [B, H, V, D_h]
        k = k.view(B, V, self.n_heads, self.head_dim).transpose(1, 2)
        v = v.view(B, V, self.n_heads, self.head_dim).transpose(1, 2)

        # 3. è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°
        attn_score = torch.matmul(q, k.transpose(-2, -1))  # [B, H, V, V]

        # 4. è½»é‡çº§RPE biasï¼ˆæ¯å¤´ç‹¬ç«‹ï¼‰
        if self.rpe_bias is not None:
            # [H, max_hop+1] -> [H, V, V] é€šè¿‡ç´¢å¼•æŸ¥æ‰¾
            rpe_bias_lookup = self.rpe_bias[:, self.hop_matrix]  # [H, V, V]
            attn_score = attn_score + rpe_bias_lookup.unsqueeze(0)  # broadcast to [B, H, V, V]

        # 5. ç»Ÿä¸€ç¼©æ”¾å¹¶softmax
        attn = (attn_score * self.scale).softmax(dim=-1)
        attn = self.attn_drop(attn)

        # 6. ç›´æ¥ä½¿ç”¨å­¦ä¹ åˆ°çš„æ³¨æ„åŠ›
        out_heads = torch.einsum('bhvw,bhwd->bhvd', attn, v)

        # 7. æ¢å¤å½¢çŠ¶å¹¶è¾“å‡ºæŠ•å½±
        out = out_heads.transpose(1, 2).reshape(B, V, D)
        out = self.out_proj(out)
        
        return out, attn


class EncoderLayer(nn.Module):
    """å•å±‚ç¼–ç å™¨"""
    
    def __init__(self, d_model, n_heads, dropout_rate=0.0, drop_path_rate=0.0, hop_matrix=None):
        super().__init__()
        self.norm1 = nn.LayerNorm(d_model)
        self.attn = AttentionLayer(
            d_model=d_model, 
            n_heads=n_heads, 
            dropout_rate=dropout_rate,
            hop_matrix=hop_matrix
        )
        self.drop_path1 = DropPath(drop_path_rate) if drop_path_rate > 0. else nn.Identity()

    def forward(self, x):
        # Pre-LNæ¶æ„
        shortcut1 = x
        x_norm1 = self.norm1(x)
        attn_output, attn_weights = self.attn(x_norm1)
        x = shortcut1 + self.drop_path1(attn_output)
        return x, attn_weights

class DynamicFusionGate(nn.Module):
    """åŠ¨æ€èåˆé—¨"""
    
    def __init__(self, feature_dim, gate_hidden_dim=None):
        super().__init__()
        if gate_hidden_dim is None:
            gate_hidden_dim = feature_dim // 4
        
        self.gate_network = nn.Sequential(
            nn.Linear(feature_dim * 2, gate_hidden_dim),
            nn.ReLU(),
            nn.LayerNorm(gate_hidden_dim),
            nn.Linear(gate_hidden_dim, feature_dim),
            nn.Sigmoid()
        )
        final_linear_layer = self.gate_network[-2]
        # å°†å…¶åç½®åˆå§‹åŒ–ä¸º0
        if final_linear_layer.bias is not None:
            nn.init.constant_(final_linear_layer.bias, 0)

    def forward(self, feat1, feat2):
        # feat1, feat2 çš„å½¢çŠ¶æ˜¯ [B, C, T, V]
        
        # 1. å°†ç»´åº¦Cæ¢åˆ°æœ€å
        feat1_p = feat1.permute(0, 2, 3, 1) # -> [B, T, V, C]
        feat2_p = feat2.permute(0, 2, 3, 1) # -> [B, T, V, C]
        
        # 2. åœ¨æœ€åä¸€ä¸ªç»´åº¦ï¼ˆCï¼‰ä¸Šæ‹¼æ¥
        combined = torch.cat([feat1_p, feat2_p], dim=-1) # -> [B, T, V, 2*C]
        
        # 3. è®¡ç®—é—¨æ§å€¼
        gate = self.gate_network(combined) # Linearä½œç”¨äº2*Cç»´åº¦
        
        # 4. è¿›è¡Œèåˆ
        fused = gate * feat1_p + (1 - gate) * feat2_p
        
        # 5. å°†ç»´åº¦æ¢å›æ¥
        fused_out = fused.permute(0, 3, 1, 2) # -> [B, C, T, V]
        
        return fused_out

# æ·»åŠ TD-GCNçš„ç©ºé—´å¤„ç†æ¨¡å—
class TDGC(nn.Module):
    """TD-GCNçš„æ ¸å¿ƒç©ºé—´å¤„ç†æ¨¡å—"""
    def __init__(self, in_channels, out_channels, rel_reduction=8, mid_reduction=1):
        super(TDGC, self).__init__()
        self.in_channels = in_channels
        self.out_channels = out_channels
        if in_channels == 3 or in_channels == 9:
            self.rel_channels = 8
            self.mid_channels = 16
        else:
            self.rel_channels = in_channels // rel_reduction
            self.mid_channels = in_channels // mid_reduction
        self.conv1 = nn.Conv2d(self.in_channels, self.rel_channels, kernel_size=1)
        self.conv3 = nn.Conv2d(self.in_channels, self.out_channels, kernel_size=1)
        self.conv4 = nn.Conv2d(self.rel_channels, self.out_channels, kernel_size=1)

        self.tanh = nn.Tanh()

        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                conv_init(m)
            elif isinstance(m, nn.BatchNorm2d):
                bn_init(m, 1)

    def forward(self, x, A=None, alpha=1, beta=1, gamma=0.1):
        x1, x3 = self.conv1(x).mean(-2), self.conv3(x)
        x1 = self.tanh(x1.unsqueeze(-1) - x1.unsqueeze(-2))
        x1 = self.conv4(x1) * alpha + (A.unsqueeze(0).unsqueeze(0) if A is not None else 0)
        x1 = torch.einsum('ncuv,nctv->nctu', x1, x3)
        x4 = self.tanh(x3.mean(-3).unsqueeze(-1) - x3.mean(-3).unsqueeze(-2))
        x3 = x3.permute(0, 2, 1, 3)
        x5 = torch.einsum('btmn,btcn->bctm', x4, x3)
        x1 = x1 * beta + x5 * gamma
        return x1


class LocalGCN(nn.Module):
    """å±€éƒ¨è·¯å¾„ï¼šä½¿ç”¨GCNå¤„ç†å±€éƒ¨ç‰©ç†è¿æ¥"""
    def __init__(self, in_channels, out_channels, A, coff_embedding=4, adaptive=True, residual=True):
        super(LocalGCN, self).__init__()
        inter_channels = out_channels // coff_embedding
        self.inter_c = inter_channels
        self.out_c = out_channels
        self.in_c = in_channels
        self.adaptive = adaptive
        self.num_subset = A.shape[0]
        self.convs = nn.ModuleList()
        for i in range(self.num_subset):
            self.convs.append(TDGC(in_channels, out_channels))

        if residual:
            if in_channels != out_channels:
                self.down = nn.Sequential(
                    nn.Conv2d(in_channels, out_channels, 1),
                    nn.BatchNorm2d(out_channels)
                )
            else:
                self.down = lambda x: x
        else:
            self.down = lambda x: 0
        if self.adaptive:
            # Aå·²ç»æ˜¯torch.Tensorï¼Œç›´æ¥è½¬æ¢ä¸ºParameter
            self.PA = nn.Parameter(A.float())
        else:
            # Aå·²ç»æ˜¯torch.Tensorï¼Œç›´æ¥ä½¿ç”¨
            self.A = Variable(A.float(), requires_grad=False)
        self.alpha = nn.Parameter(torch.zeros(1))
        self.bn = nn.BatchNorm2d(out_channels)
        self.relu = nn.ReLU(inplace=True)

        self.beta = nn.Parameter(torch.tensor(0.5))
        self.gamma = nn.Parameter(torch.tensor(0.1))

        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                conv_init(m)
            elif isinstance(m, nn.BatchNorm2d):
                bn_init(m, 1)
        bn_init(self.bn, 1e-6)

    def forward(self, x):
        y = None
        if self.adaptive:
            A = self.PA
        else:
            # ç¡®ä¿Aåœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
            A = self.A.to(x.device)
        for i in range(self.num_subset):
            z = self.convs[i](x, A[i], self.alpha, self.beta, self.gamma)
            y = z + y if y is not None else z
        y = self.bn(y)
        y += self.down(x)
        y = self.relu(y)
        return y


class DynamicDualStreamAttention(nn.Module):
    """ç©ºé—´æ³¨æ„åŠ› - ã€åˆ›æ–°ç‚¹ï¼šå±€éƒ¨GCN + å…¨å±€Transformer + åŠ¨æ€èåˆã€‘"""
    def __init__(self, d_model,
                 A=None,  # æ·»åŠ Aå‚æ•°
                 hop_matrix=None,
                 # å±€éƒ¨è·¯å¾„å‚æ•°ï¼ˆGCNï¼‰
                 local_gcn_layers=1,
                 # å…¨å±€è·¯å¾„å‚æ•°ï¼ˆTransformerï¼‰
                 global_st_layers=1, global_n_heads=4,
                 # èåˆå‚æ•°
                 fusion_type='dynamic_gate', fusion_gate_hidden_dim=None,
                 dropout_rate=0.0, drop_path_rate=0.0, **kwargs):
        super().__init__()
        
        self.d_model = d_model
        self.fusion_type = fusion_type
        self.dropout_rate = dropout_rate
        
        # --- 1. å±€éƒ¨è·¯å¾„ï¼šGCNå¤„ç† (æ›¿æ¢åŸæ¥çš„å±€éƒ¨æ³¨æ„åŠ›) ---
        self.local_gcn_layers = nn.ModuleList()
        for i in range(local_gcn_layers):
            # æ¯ä¸ªGCNå±‚éƒ½éœ€è¦Aå‚æ•°
            self.local_gcn_layers.append(
                LocalGCN(d_model, d_model, A, adaptive=True, residual=True)
            )
        
        # --- 2. å…¨å±€è·¯å¾„ï¼šTransformerå¤„ç† (ä¿æŒä¸å˜) ---
        self.global_layers = nn.ModuleList()
        for i in range(global_st_layers):
            self.global_layers.append(
                EncoderLayer(
                    d_model=d_model,
                    n_heads=global_n_heads,
                    dropout_rate=dropout_rate,
                    drop_path_rate=drop_path_rate,
                    hop_matrix=hop_matrix
                )
            )
        
        # èåˆç­–ç•¥ - ä¿æŒåŠ¨æ€èåˆåˆ›æ–°ç‚¹
        if fusion_type == 'dynamic_gate':
            self.fusion = DynamicFusionGate(d_model, fusion_gate_hidden_dim)
        elif fusion_type == 'add':
            self.fusion = lambda x, y: x + y
        elif fusion_type == 'concat':
            self.fusion = lambda x, y: torch.cat([x, y], dim=-1)
            self.proj = nn.Linear(d_model * 2, d_model)
        else:
            raise ValueError(f"Unknown fusion type: {fusion_type}")

    def forward(self, x):
        # xæ˜¯4D: [B, C, T, V]
        B, C, T, V = x.shape
        
        # 1. å±€éƒ¨è·¯å¾„å¤„ç†ï¼ˆGCNï¼‰- ç›´æ¥ä½¿ç”¨4Dè¾“å…¥
        local_output = x
        for layer in self.local_gcn_layers:
            local_output = layer(local_output)
        
        # 2. å…¨å±€è·¯å¾„å¤„ç†ï¼ˆTransformerï¼‰- éœ€è¦è½¬æ¢ä¸º3D
        # ä»4D [B, C, T, V] è½¬æ¢ä¸º3D [B*T, V, C]
        x_3d = rearrange(x, 'b c t v -> (b t) v c')
        
        global_output = x_3d
        for layer in self.global_layers:
            global_output, attn_weights = layer(global_output)
        
        # å°†Transformerè¾“å‡ºè½¬æ¢å›4Dæ ¼å¼ç”¨äºèåˆ
        global_output_4d = rearrange(global_output, '(b t) v c -> b c t v', b=B, t=T)

        # 3. èåˆ
        if self.fusion_type == 'add':
            # ç›¸åŠ èåˆï¼šä»…åœ¨ add åˆ†æ”¯åšè¿ç»­åŒ–ï¼Œé¿å… out= ä»¥æ”¯æŒè‡ªåŠ¨æ±‚å¯¼
            local_tensor = local_output.contiguous()
            global_tensor = global_output_4d.contiguous()
            fused_output = torch.add(local_tensor, global_tensor)
        elif self.fusion_type == 'concat':
            fused_output = self.fusion(local_output, global_output_4d)
            fused_output = self.proj(fused_output)
        else:
            fused_output = self.fusion(local_output, global_output_4d)
        
        return fused_output, attn_weights

class TemporalConv(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size, stride=1, dilation=1):
        super(TemporalConv, self).__init__()
        pad = (kernel_size + (kernel_size - 1) * (dilation - 1) - 1) // 2
        self.conv = nn.Conv2d(
            in_channels,
            out_channels,
            kernel_size=(kernel_size, 1),
            padding=(pad, 0),
            stride=(stride, 1),
            dilation=(dilation, 1))

        self.bn = nn.BatchNorm2d(out_channels)

    def forward(self, x):
        x = self.conv(x)
        x = self.bn(x)
        return x


class MultiScale_TemporalConv(nn.Module):
    def __init__(self,
                 in_channels,
                 out_channels,
                 kernel_size=5,
                 stride=1,
                 dilations=[1,2],
                 residual=True,
                 residual_kernel_size=1):

        super().__init__()
        assert out_channels % (len(dilations) + 2) == 0, '# out channels should be multiples of # branches'

        # Multiple branches of temporal convolution
        self.num_branches = len(dilations) + 2
        branch_channels = out_channels // self.num_branches
        if type(kernel_size) == list:
            assert len(kernel_size) == len(dilations)
        else:
            kernel_size = [kernel_size] * len(dilations)
        # Temporal Convolution branches
        self.branches = nn.ModuleList([
            nn.Sequential(
                nn.Conv2d(
                    in_channels,
                    branch_channels,
                    kernel_size=1,
                    padding=0),
                nn.BatchNorm2d(branch_channels),
                nn.ReLU(inplace=True),
                TemporalConv(
                    branch_channels,
                    branch_channels,
                    kernel_size=ks,
                    stride=stride,
                    dilation=dilation),
            )
            for ks, dilation in zip(kernel_size, dilations)
        ])

        # Additional Max & 1x1 branch
        self.branches.append(nn.Sequential(
            nn.Conv2d(in_channels, branch_channels, kernel_size=1, padding=0),
            nn.BatchNorm2d(branch_channels),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=(3, 1), stride=(stride, 1), padding=(1, 0)),
            nn.BatchNorm2d(branch_channels)
        ))

        self.branches.append(nn.Sequential(
            nn.Conv2d(in_channels, branch_channels, kernel_size=1, padding=0, stride=(stride, 1)),
            nn.BatchNorm2d(branch_channels)
        ))

        # Residual connection
        if not residual:
            self.residual = lambda x: 0
        elif (in_channels == out_channels) and (stride == 1):
            self.residual = lambda x: x
        else:
            self.residual = TemporalConv(in_channels, out_channels, kernel_size=residual_kernel_size, stride=stride)

        # initialize
        self.apply(weights_init)

    def forward(self, x):
        branch_outs = []
        for tempconv in self.branches:
            out = tempconv(x)
            branch_outs.append(out)

        out = torch.cat(branch_outs, dim=1)
        out += self.residual(x)
        return out


class SpatioTemporalUnit(nn.Module):
    def __init__(self, in_channels, out_channels, A=None, hop_matrix=None,
                 stride=1, spatial_config=None, residual=True):
        super().__init__()
        
        self.in_channels = in_channels
        self.out_channels = out_channels
        self.stride = stride

        # --- 1. ç©ºé—´å¤„ç†æ¨¡å— (ç±»ä¼¼TCN_GCN_unitä¸­çš„gcn1) ---
        if spatial_config is None:
            spatial_config = {
                'local_gcn_layers': 1,
                'global_st_layers': 1,
                'fusion_type': 'dynamic_gate',
                'fusion_gate_hidden_dim': None,
                'dropout_rate': 0.0,
            }

        # è‡ªåŠ¨è®¡ç®—å…¨å±€è·¯å¾„çš„å¤´æ•°ï¼ˆå±€éƒ¨è·¯å¾„ä½¿ç”¨GCNï¼Œä¸éœ€è¦å¤šå¤´æ³¨æ„åŠ›ï¼‰
        spatial_config['global_n_heads'] = 4
        
        self.spatial_attn = DynamicDualStreamAttention(
            d_model=in_channels,
            A=A,  # æ·»åŠ Aå‚æ•°ä¼ é€’ç»™LocalGCN
            hop_matrix=hop_matrix,  
            **spatial_config
        )
        
        # --- 2. æ—¶é—´å¤„ç†æ¨¡å— (ç±»ä¼¼TCN_GCN_unitä¸­çš„tcn1) ---
        # ç›´æ¥ä½¿ç”¨MultiScale_TemporalConvçš„é»˜è®¤å‚æ•°
        self.temporal_conv = MultiScale_TemporalConv(
            in_channels, in_channels,  # ä¿æŒé€šé“æ•°ä¸å˜
            kernel_size=5,  # é»˜è®¤å€¼
            stride=stride,
            dilations=[1, 2],  # é»˜è®¤å€¼
            residual=False  # åœ¨å•å…ƒå†…éƒ¨ä¸ä½¿ç”¨æ®‹å·®ï¼Œåœ¨å•å…ƒçº§åˆ«ä½¿ç”¨
        )
        
        # --- 3. æ®‹å·®è¿æ¥å¤„ç† (ç±»ä¼¼TCN_GCN_unitä¸­çš„residual) ---
        self.relu = nn.ReLU(inplace=True)
        
        if not residual:
            self.residual = lambda x: 0
        elif (in_channels == out_channels) and (stride == 1):
            self.residual = lambda x: x
        else:
            # éœ€è¦ä¸‹é‡‡æ ·æˆ–é€šé“æ•°å˜åŒ–æ—¶çš„æ®‹å·®è¿æ¥
            self.residual = nn.Sequential(
                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=(stride, 1)),
                nn.BatchNorm2d(out_channels)
            )
        
        # --- 4. é€šé“æŠ•å½± (å¦‚æœéœ€è¦) ---
        if in_channels != out_channels:
            self.channel_proj = nn.Conv2d(in_channels, out_channels, 1)
        else:
            self.channel_proj = nn.Identity()

    def forward(self, x):
        # ä¿å­˜è¾“å…¥ç”¨äºæ®‹å·®è¿æ¥
        residual = x
        
        # --- 1. ç©ºé—´å¤„ç† ---
        # ç›´æ¥ä¼ é€’4Dæ•°æ®ç»™spatial_attnï¼Œå®ƒä¼šå†…éƒ¨å¤„ç†GCNå’ŒTransformerçš„ç»´åº¦éœ€æ±‚
        x_spatial, _ = self.spatial_attn(x)
        
        # --- 2. æ—¶é—´å¤„ç† ---
        x_temporal = self.temporal_conv(x_spatial)
        
        # --- 3. é€šé“æŠ•å½± ---
        x_out = self.channel_proj(x_temporal)
        
        # --- 4. æ®‹å·®è¿æ¥å’Œæ¿€æ´» (ç±»ä¼¼TCN_GCN_unitçš„æœ€ç»ˆå¤„ç†) ---
        y = self.relu(x_out + self.residual(residual))
        
        return y

# === ä¸»æ¨¡å‹ï¼šSDT Transformer ===
class Model(nn.Module):
    def __init__(self, model_cfg: dict):
        super().__init__()
        
        # åŸºç¡€å‚æ•°
        self.in_channels = 3  # ç›´æ¥å®šæ­»ä¸º3ï¼Œå¯¹åº”éª¨æ¶çš„x,y,zåæ ‡
        self.num_nodes = model_cfg['num_nodes']
        self.num_class = model_cfg['num_class']
        self.num_person = model_cfg.get('num_person', 2)
        
        # ğŸ”¥ã€ä¿®æ­£3 & 4ã€‘åœ¨é¡¶å±‚å®šä¹‰æ­£åˆ™åŒ–å‚æ•°
        self.dropout_rate = model_cfg.get('dropout_rate', 0.1)
        self.drop_path_rate = model_cfg.get('drop_path_rate', 0.1)
        
        # æ·»åŠ æ•°æ®å½’ä¸€åŒ–å±‚
        self.data_bn = nn.BatchNorm1d(self.num_person * self.in_channels * self.num_nodes)
        
        # æ¨¡å‹ç»´åº¦ - ç›´æ¥å®šæ­»base_channelsä¸º64
        self.base_channels = 64
        
        # å›¾é…ç½®
        graph = model_cfg.get('graph', None)
        graph_args = model_cfg.get('graph_args', {})

        if graph is None:
            raise ValueError("å¿…é¡»æä¾›graphå‚æ•°ï¼Œä¾‹å¦‚ 'graph.ntu_rgb_d.Graph'")
        else:
            Graph = import_class(graph)
            self.graph = Graph(**graph_args)
            
        A = torch.from_numpy(self.graph.A).float() # (K, V, V) e.g. (3, 25, 25)

        # å‡†å¤‡ç”¨äºã€å…¨å±€æ³¨æ„åŠ›ã€‘çš„è·³æ•°è·ç¦»çŸ©é˜µ (V, V)
        # ä½¿ç”¨é¢„è®¡ç®—çš„è·³æ•°çŸ©é˜µï¼Œæ ¹æ®æ•°æ®é›†åç§°è·å–
        dataset_name = model_cfg.get('dataset_name', 'ntu_rgb_d')
        hop_matrix = get_precomputed_hop_matrix(dataset_name, device='cpu')
        self.register_buffer('hop_matrix', hop_matrix)
        
        # è¾“å…¥æŠ•å½±
        self.input_proj = nn.Conv2d(self.in_channels, self.base_channels, kernel_size=1)
        
        # ç©ºé—´æ³¨æ„åŠ›é…ç½®
        spatial_config = {
            'local_gcn_layers': model_cfg.get('sa_local_gcn_layers', 1),
            'global_st_layers': model_cfg.get('sa_global_st_layers', 1),
            'fusion_type': model_cfg.get('spatial_fusion_type', 'dynamic_gate'),
            'fusion_gate_hidden_dim': model_cfg.get('spatial_fusion_gate_hidden_dim', None),
            'dropout_rate': self.dropout_rate,
            'drop_path_rate': self.drop_path_rate,  
        }
        
        # ç›´æ¥å®šä¹‰l1åˆ°l10å±‚ï¼Œå‚è€ƒHD-GCNçš„æ–¹å¼
        # ç¬¬1é˜¶æ®µï¼šåŸºç¡€ç‰¹å¾æå– (l1-l4, stride=1)
        self.l1 = SpatioTemporalUnit(self.base_channels, self.base_channels,
                                    A=A,  # æ·»åŠ Aå‚æ•°
                                    hop_matrix=self.hop_matrix,
                                    spatial_config=spatial_config,
                                    residual=False)
        self.l2 = SpatioTemporalUnit(self.base_channels, self.base_channels,
                                    A=A,  # æ·»åŠ Aå‚æ•°
                                    hop_matrix=self.hop_matrix,
                                    spatial_config=spatial_config)
        self.l3 = SpatioTemporalUnit(self.base_channels, self.base_channels,
                                    A=A,  # æ·»åŠ Aå‚æ•°
                                    hop_matrix=self.hop_matrix,
                                    spatial_config=spatial_config)
        self.l4 = SpatioTemporalUnit(self.base_channels, self.base_channels,
                                    A=A,  # æ·»åŠ Aå‚æ•°
                                    hop_matrix=self.hop_matrix,
                                    spatial_config=spatial_config)
        
        # ç¬¬2é˜¶æ®µï¼šä¸‹é‡‡æ · + ç‰¹å¾å¢å¼º (l5-l7)
        self.l5 = SpatioTemporalUnit(self.base_channels, self.base_channels*2,
                                    A=A,  # æ·»åŠ Aå‚æ•°
                                    hop_matrix=self.hop_matrix,
                                    stride=2,
                                    spatial_config=spatial_config)
        self.l6 = SpatioTemporalUnit(self.base_channels*2, self.base_channels*2,
                                    A=A,  # æ·»åŠ Aå‚æ•°
                                    hop_matrix=self.hop_matrix,
                                    spatial_config=spatial_config)
        self.l7 = SpatioTemporalUnit(self.base_channels*2, self.base_channels*2,
                                    A=A,  # æ·»åŠ Aå‚æ•°
                                    hop_matrix=self.hop_matrix,
                                    spatial_config=spatial_config)
        
        # ç¬¬3é˜¶æ®µï¼šé«˜çº§ç‰¹å¾æå– (l8-l10)
        self.l8 = SpatioTemporalUnit(self.base_channels*2, self.base_channels*4,
                                    A=A,  # æ·»åŠ Aå‚æ•°
                                    hop_matrix=self.hop_matrix,
                                    stride=2,
                                    spatial_config=spatial_config)
        self.l9 = SpatioTemporalUnit(self.base_channels*4, self.base_channels*4,
                                    A=A,  # æ·»åŠ Aå‚æ•°
                                    hop_matrix=self.hop_matrix,
                                    spatial_config=spatial_config)
        self.l10 = SpatioTemporalUnit(self.base_channels*4, self.base_channels*4,
                                     A=A,  # æ·»åŠ Aå‚æ•°
                                     hop_matrix=self.hop_matrix,
                                     spatial_config=spatial_config)

        # åˆ†ç±»å¤´
        self.fc = nn.Linear(self.base_channels*4, self.num_class)

        nn.init.normal_(self.fc.weight, 0, math.sqrt(2. / self.num_class))
        bn_init(self.data_bn, 1)
        if self.dropout_rate > 0:
            self.drop_out = nn.Dropout(self.dropout_rate)
        else:
            self.drop_out = lambda x: x

    def forward(self, x):
        """
        è¾“å…¥:
            x: éª¨æ¶åºåˆ— (B, C, T, V, M) -        
        è¿”å›:
            logits: (B, num_class)
        """
        if len(x.shape) == 3:
            # è¾“å…¥æ ¼å¼: (B, T, V*C)
            B, T, VC = x.shape
            x = x.view(B, T, self.num_nodes, -1).permute(0, 3, 1, 2).contiguous().unsqueeze(-1)
        elif len(x.shape) == 4:
            # è¾“å…¥æ ¼å¼: (B, T, V, C) -> (B, C, T, V, 1)
            B, T, V, C = x.shape
            x = x.permute(0, 3, 1, 2).contiguous().unsqueeze(-1)
        B, C, T, V, M = x.size()
        
        # æ•°æ®å½’ä¸€åŒ–ï¼ˆå…ˆå½’ä¸€åŒ–ï¼‰
        x = x.permute(0, 4, 3, 1, 2).contiguous().view(B, M * V * C, T)
        x = self.data_bn(x)
        x = x.view(B, M, V, C, T).permute(0, 1, 3, 4, 2).contiguous().view(B * M, C, T, V)
        
        # è¾“å…¥æŠ•å½±ï¼ˆå†æŠ•å½±ï¼‰
        x = self.input_proj(x)
        # æ—¶ç©ºèåˆå¤„ç† - ç›´æ¥è°ƒç”¨l1åˆ°l10ï¼Œå‚è€ƒHD-GCNçš„æ–¹å¼
        x = self.l1(x)
        x = self.l2(x)
        x = self.l3(x)
        x = self.l4(x)
        x = self.l5(x)
        x = self.l6(x)
        x = self.l7(x)
        x = self.l8(x)
        x = self.l9(x)
        x = self.l10(x)

        # ç®€åŒ–çš„æ± åŒ–å’Œåˆ†ç±»
        _, C_final, T, V = x.size()
        x = x.view(B, M, C_final, -1)
        x = x.mean(3).mean(1)  # ç›´æ¥ä½¿ç”¨å¹³å‡æ± åŒ–
        x = self.drop_out(x)
        logits = self.fc(x)   
        return logits 